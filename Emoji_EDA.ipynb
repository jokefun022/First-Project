{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/jokefun022/blob/main/Emoji_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xMeIAqcqSw"
      },
      "source": [
        "# Emoji Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYuYYqmcqSx"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CdqgMiZ_cqSx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaFC6dfUkiXL",
        "outputId": "3a6771b7-85c2-4870-a9b5-476a54919b14"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Complete Data With Emoji.csv',)\n",
        "                 #encoding='utf-8')\n",
        "\n",
        "\n",
        "#from IPython.display import display\n",
        "\n",
        "#display(df)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(df.tail())\n",
        "\n",
        "display(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "BPR8Wi2Qo4-W",
        "outputId": "c556e617-d69c-4022-bbce-74da4865e6f2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               Tweet_Text_With_Emoji  Label Sentiment Analysis\n",
            "0                             kah kay yih sach ðŸ˜Š hai      0            Neutral\n",
            "1  neither bhadwaa ðŸ˜‘ tantra rather madarjaat log ...      0            Neutral\n",
            "2          kyoon okay yih kuchh logo ki ðŸ˜ƒ fitrat hai      0            Neutral\n",
            "3  lagta hsi man ðŸ™Œ chaaha job naey day rahay bhaa...      0            Neutral\n",
            "4                achchha na bataao mujhay pata hai ðŸ˜Š      0            Neutral\n",
            "                                   Tweet_Text_With_Emoji  Label  \\\n",
            "10667  hope sort male society discrimination mullah h...      4   \n",
            "10668          ahmadiya are also citizen without ðŸ‘¿ right      4   \n",
            "10669  dear marathon relation discussion ahmedis know...      4   \n",
            "10670  halala ki paedaaish andha hai buddddhay dikh n...      4   \n",
            "10671  inh koh majaa aata hai tabhiii ðŸ¤¬ nahi mangii h...      4   \n",
            "\n",
            "      Sentiment Analysis  \n",
            "10667     Religious Hate  \n",
            "10668     Religious Hate  \n",
            "10669     Religious Hate  \n",
            "10670     Religious Hate  \n",
            "10671     Religious Hate  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                   Tweet_Text_With_Emoji  Label  \\\n",
              "0                                 kah kay yih sach ðŸ˜Š hai      0   \n",
              "1      neither bhadwaa ðŸ˜‘ tantra rather madarjaat log ...      0   \n",
              "2              kyoon okay yih kuchh logo ki ðŸ˜ƒ fitrat hai      0   \n",
              "3      lagta hsi man ðŸ™Œ chaaha job naey day rahay bhaa...      0   \n",
              "4                    achchha na bataao mujhay pata hai ðŸ˜Š      0   \n",
              "...                                                  ...    ...   \n",
              "10667  hope sort male society discrimination mullah h...      4   \n",
              "10668          ahmadiya are also citizen without ðŸ‘¿ right      4   \n",
              "10669  dear marathon relation discussion ahmedis know...      4   \n",
              "10670  halala ki paedaaish andha hai buddddhay dikh n...      4   \n",
              "10671  inh koh majaa aata hai tabhiii ðŸ¤¬ nahi mangii h...      4   \n",
              "\n",
              "      Sentiment Analysis  \n",
              "0                Neutral  \n",
              "1                Neutral  \n",
              "2                Neutral  \n",
              "3                Neutral  \n",
              "4                Neutral  \n",
              "...                  ...  \n",
              "10667     Religious Hate  \n",
              "10668     Religious Hate  \n",
              "10669     Religious Hate  \n",
              "10670     Religious Hate  \n",
              "10671     Religious Hate  \n",
              "\n",
              "[10672 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f181fad6-27b6-4b04-95a2-1964d991be92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_Text_With_Emoji</th>\n",
              "      <th>Label</th>\n",
              "      <th>Sentiment Analysis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kah kay yih sach ðŸ˜Š hai</td>\n",
              "      <td>0</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neither bhadwaa ðŸ˜‘ tantra rather madarjaat log ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kyoon okay yih kuchh logo ki ðŸ˜ƒ fitrat hai</td>\n",
              "      <td>0</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lagta hsi man ðŸ™Œ chaaha job naey day rahay bhaa...</td>\n",
              "      <td>0</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>achchha na bataao mujhay pata hai ðŸ˜Š</td>\n",
              "      <td>0</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10667</th>\n",
              "      <td>hope sort male society discrimination mullah h...</td>\n",
              "      <td>4</td>\n",
              "      <td>Religious Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10668</th>\n",
              "      <td>ahmadiya are also citizen without ðŸ‘¿ right</td>\n",
              "      <td>4</td>\n",
              "      <td>Religious Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10669</th>\n",
              "      <td>dear marathon relation discussion ahmedis know...</td>\n",
              "      <td>4</td>\n",
              "      <td>Religious Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10670</th>\n",
              "      <td>halala ki paedaaish andha hai buddddhay dikh n...</td>\n",
              "      <td>4</td>\n",
              "      <td>Religious Hate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10671</th>\n",
              "      <td>inh koh majaa aata hai tabhiii ðŸ¤¬ nahi mangii h...</td>\n",
              "      <td>4</td>\n",
              "      <td>Religious Hate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10672 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f181fad6-27b6-4b04-95a2-1964d991be92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f181fad6-27b6-4b04-95a2-1964d991be92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f181fad6-27b6-4b04-95a2-1964d991be92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3cf0d2dc-0b63-4ade-8f7e-04b0a07dd297\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3cf0d2dc-0b63-4ade-8f7e-04b0a07dd297')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3cf0d2dc-0b63-4ade-8f7e-04b0a07dd297 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e19d16a4-07ab-47c6-ad3f-f750f27c9858\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e19d16a4-07ab-47c6-ad3f-f750f27c9858 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10672,\n  \"fields\": [\n    {\n      \"column\": \"Tweet_Text_With_Emoji\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10577,\n        \"samples\": [\n          \"sun any ttoh majedar lag raha delhi spensorrr spensorrr \\ud83d\\ude0e\",\n          \"haraam kay pille tairi maan any tujhay sikhaaya nahi kya kaesay ladkiyon kay bare main baat kartay hainmujhe lagta hai tairay ghar main tairay maai baap nahi hain tabhi kuttay ki tarh bhaunk raha haighar par bhi apni bahan ko nangi naachne vaali bhando bolta hai kya \\ud83d\\udd95\",\n          \"ratt koi physics samjha day bahanchod \\ud83d\\ude07\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment Analysis\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Racism\",\n          \"Religious Hate\",\n          \"Abusive/Offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRjR6k9RcqSy"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3261431",
        "outputId": "755bac88-f8e4-4fb9-d12c-55f4514513af"
      },
      "source": [
        "# Count the occurrences of each sentiment category\n",
        "sentiment_counts = df['Sentiment Analysis'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts of comments by Sentiment Analysis category:\")\n",
        "print(sentiment_counts)\n",
        "\n",
        "# You can also explicitly check for 'Abusive/Offensive'\n",
        "abusive_count = sentiment_counts.get('Abusive/Offensive', 0)\n",
        "print(f\"\\nNumber of Abusive/Offensive comments: {abusive_count}\")\n",
        "\n",
        "# Assuming all other categories are non-abusive for this purpose\n",
        "non_abusive_count = df.shape[0] - abusive_count\n",
        "print(f\"Number of Non-Abusive comments: {non_abusive_count}\")\n",
        "\n",
        "\n",
        "# Categorize into Positive, Negative, and Neutral\n",
        "def categorize_sentiment(sentiment):\n",
        "    if sentiment == 'Neutral':\n",
        "        return 'Neutral'\n",
        "    elif sentiment in ['Abusive/Offensive', 'Racism', 'Sexium', 'Religious  Hate']:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Positive' # Assuming any other category is positive, adjust if needed\n",
        "\n",
        "df['Broad_Sentiment'] = df['Sentiment Analysis'].apply(categorize_sentiment)\n",
        "\n",
        "# Count the occurrences of the broad sentiment categories\n",
        "broad_sentiment_counts = df['Broad_Sentiment'].value_counts()\n",
        "\n",
        "print(\"\\nCounts of comments by Broad Sentiment category:\")\n",
        "print(broad_sentiment_counts)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of comments by Sentiment Analysis category:\n",
            "Sentiment Analysis\n",
            "Neutral              6055\n",
            "Abusive/Offensive    3141\n",
            "Racism                725\n",
            "Sexism                398\n",
            "Religious Hate        353\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Number of Abusive/Offensive comments: 3141\n",
            "Number of Non-Abusive comments: 7531\n",
            "\n",
            "Counts of comments by Broad Sentiment category:\n",
            "Broad_Sentiment\n",
            "Neutral     6055\n",
            "Negative    3866\n",
            "Positive     751\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  counted = (series['Label']\n",
        "                .value_counts()\n",
        "              .reset_index(name='counts')\n",
        "              .rename({'index': 'Label'}, axis=1)\n",
        "              .sort_values('Label', ascending=True))\n",
        "  xs = counted['Label']\n",
        "  ys = counted['counts']\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = _df_2.sort_values('Label', ascending=True)\n",
        "for i, (series_name, series) in enumerate(df_sorted.groupby('Sentiment Analysis')):\n",
        "  _plot_series(series, series_name, i)\n",
        "  fig.legend(title='Sentiment Analysis', bbox_to_anchor=(1, 1), loc='upper left')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('Label')\n",
        "_ = plt.ylabel('count()')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lLS1KkP-iMbI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "0ed8f11a-ccc3-40cf-e06e-766e9e4ec016"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_df_2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-304585113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constrained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_df_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseries_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment Analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0m_plot_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_df_2' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x520 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['Label'].plot(kind='hist', bins=20, title='Label')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ISRgWoyTiLQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "_df_1.groupby('Sentiment Analysis').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sb7QXJGwiH48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyudsjxcqS0"
      },
      "source": [
        "### Emoji mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pdipcwrcqS0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=\"Label\", data=df, palette=\"plasma\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHvGqk0ZcqS0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "df[\"Length\"] = df[\"Tweet_Text_With_Emoji\"].apply(len)\n",
        "sns.distplot(df[\"Length\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic7WnUE0cqS1"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5474de41"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf1c31c8"
      },
      "source": [
        "import re\n",
        "\n",
        "# Function to check if a string contains an emoji\n",
        "def contains_emoji(text):\n",
        "    # This regex pattern matches common emoji ranges\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n",
        "        \"\\U0001F700-\\U0001F77F\"  # Alphanumeric\n",
        "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
        "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows\n",
        "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\"\n",
        "    )\n",
        "    return bool(emoji_pattern.search(text))\n",
        "\n",
        "# Filter the DataFrame to get comments without emojis\n",
        "comments_without_emoji = df[~df['Tweet_Text_With_Emoji'].apply(contains_emoji)]\n",
        "\n",
        "# Display the filtered comments\n",
        "display(comments_without_emoji)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cad65895"
      },
      "source": [
        "# Filter the DataFrame to get comments with emojis\n",
        "comments_with_emoji = df[df['Tweet_Text_With_Emoji'].apply(contains_emoji)]\n",
        "\n",
        "# Display the filtered comments\n",
        "display(comments_with_emoji)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bb9f53"
      },
      "source": [
        "# Task\n",
        "Analyze the data in \"/content/Complete Data With Emoji.xlsx\" to identify comments that contain emojis and comments that do not contain emojis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991ea46a"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "### Subtask:\n",
        "Preprocess the text data by cleaning and tokenizing it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2882527a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a function to clean the text by converting to lowercase, removing URLs, mentions, hashtags, punctuation, and numbers, and stripping whitespace. Then I will apply this function to the 'Tweet_Text_With_Emoji' column to create the 'cleaned_text' column. Finally, I will tokenize the cleaned text and store the tokens in the 'tokens' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfe79c73"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = text.strip()                  # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['Tweet_Text_With_Emoji'].apply(clean_text)\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "display(df[['Tweet_Text_With_Emoji', 'cleaned_text', 'tokens']].head(100000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47bc3c1"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Extract features from the text data that can be used to train a model. This could include techniques like TF-IDF or word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a01c5ad4"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit the TfidfVectorizer to the cleaned text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6a6150c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11285b8"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Choose an appropriate model for the emoji prediction task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7a8145f"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a Logistic Regression model on the TF-IDF features to predict the emoji label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee23764"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into training and testing sets, instantiate a Logistic Regression model, and train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ccd3625"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "270b443f"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained Logistic Regression model using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebc7040"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained Logistic Regression model using accuracy, classification report, and confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "442b0ca4"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "325bb0f9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Logistic Regression model achieved an overall accuracy of approximately 88.06% in predicting whether comments contain emojis.\n",
        "*   The model performed very well in identifying comments in Class 0 (precision 0.86, recall 0.99, F1-score 0.92) and Class 2 (precision 0.94, recall 0.87, F1-score 0.90).\n",
        "*   The model's performance was significantly lower for Classes 1, 3, and 4, primarily due to lower recall scores (0.47 for Class 1, 0.35 for Class 3, and 0.41 for Class 4). This indicates the model is missing a substantial number of comments belonging to these classes.\n",
        "*   The confusion matrix shows frequent misclassifications of instances from Classes 1, 3, and 4 into other classes, particularly Class 0.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the characteristics of comments in Classes 1, 3, and 4 to understand why the model struggles to identify them accurately. This could involve examining the text content or other features.\n",
        "*   Consider exploring techniques to address the class imbalance, if present, as it might be contributing to the lower recall in certain classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "367de3e3"
      },
      "source": [
        "# Task\n",
        "Improve the accuracy, precision, recall, and F1 score of the classification model using the data in \"/content/Complete Data With Emoji.xlsx\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fe3967c"
      },
      "source": [
        "## Analyze misclassifications\n",
        "\n",
        "### Subtask:\n",
        "Examine the comments that were misclassified, especially those in Classes 1, 3, and 4, to understand common patterns or features that the model might be missing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a04254e"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a DataFrame with test data, true labels, and predicted labels, then filter for misclassified comments, focusing on classes 1, 3, and 4, and display a sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc22cc11"
      },
      "source": [
        "# Create a DataFrame with test data, true labels, and predicted labels\n",
        "results_df = pd.DataFrame({\n",
        "    'text': tfidf_vectorizer.inverse_transform(X_test), # Get original text back (approximate)\n",
        "    'true_label': y_test,\n",
        "    'predicted_label': y_pred\n",
        "})\n",
        "\n",
        "# Add the original 'Tweet_Text_With_Emoji' to the results_df based on the index\n",
        "results_df['original_text'] = df.loc[results_df.index, 'Tweet_Text_With_Emoji']\n",
        "\n",
        "\n",
        "# Filter for misclassified comments\n",
        "misclassified_df = results_df[results_df['true_label'] != results_df['predicted_label']]\n",
        "\n",
        "# Filter for misclassified comments in classes 1, 3, and 4\n",
        "misclassified_target_classes = misclassified_df[misclassified_df['true_label'].isin([1, 3, 4])]\n",
        "\n",
        "# Display a sample of misclassified comments\n",
        "display(misclassified_target_classes[['original_text', 'true_label', 'predicted_label']].head(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c788a44"
      },
      "source": [
        "## Address class imbalance\n",
        "\n",
        "### Subtask:\n",
        "Address class imbalance by resampling the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ca7313"
      },
      "source": [
        "**Reasoning**:\n",
        "Import RandomOverSampler and apply it to the training data to address class imbalance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b0a61cb"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train = X_train_resampled\n",
        "y_train = y_train_resampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccc3ac17"
      },
      "source": [
        "## Experiment with different models\n",
        "\n",
        "### Subtask:\n",
        "Train a Support Vector Machine (SVM) model on the resampled TF-IDF features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143561d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the SVC class, instantiate an SVC model, and fit it to the resampled training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "786d1930"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets, svm\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Run classifier, using a model that is too regularized (C too low) to see\n",
        "# the impact on the results\n",
        "classifier = svm.SVC(kernel=\"linear\", C=0.01).fit(X_train, y_train)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "titles_options = [\n",
        "    (\"Confusion matrix, without normalization\", None),\n",
        "    (\"Normalized confusion matrix\", \"true\"),\n",
        "]\n",
        "for title, normalize in titles_options:\n",
        "    disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        classifier,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        display_labels=class_names,\n",
        "        cmap=plt.cm.Blues,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    disp.ax_.set_title(title)\n",
        "\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mt8RuD4jF2HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb12dd80"
      },
      "source": [
        "## Re-evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained SVM model using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3467f47"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained SVM model using accuracy, classification report, and confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dbce6e5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
        "\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"\\nSVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e6aa652"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Tune the hyperparameters of the trained SVM model to potentially improve its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40535828"
      },
      "source": [
        "**Reasoning**:\n",
        "Tune the hyperparameters of the trained SVM model to potentially improve its performance using GridSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed4b9305"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "grid_search = GridSearchCV(svm_model, param_grid, scoring='f1_weighted', cv=5)\n",
        "\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation F1-weighted score:\", grid_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9024746a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'df' is your DataFrame with 'cleaned_text' and 'Label' columns\n",
        "\n",
        "# 1. Prepare the data for deep learning\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000) # Adjust num_words as needed\n",
        "tokenizer.fit_on_texts(df['cleaned_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_sequence_length = 100 # Adjust based on your text length distribution\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Build the LSTM model\n",
        "embedding_dim = 100 # Adjust embedding dimension as needed\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_lstm.add(LSTM(128, return_sequences=True)) # Adjust LSTM units as needed\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(LSTM(64)) # Adjust LSTM units as needed\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Dense(len(label_encoder.classes_), activation='softmax')) # Output layer with number of classes\n",
        "\n",
        "# 3. Compile the model\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history = model_lstm.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 5. Evaluate the model (optional, but recommended after training)\n",
        "loss, accuracy = model_lstm.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64f221f6"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_prob_dl = model_lstm.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_dl = np.argmax(y_pred_prob_dl, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cell\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded = label_encoder.inverse_transform(y_pred_dl)\n",
        "\n",
        "print(\"\\nLSTM Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded))\n",
        "\n",
        "print(\"\\nLSTM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf4ae8c7"
      },
      "source": [
        "# Create a DataFrame with test data, true labels, and predicted labels for the LSTM model\n",
        "results_df_lstm = pd.DataFrame({\n",
        "    'original_text': df.loc[results_df.index, 'Tweet_Text_With_Emoji'], # Use the same index as the test set\n",
        "    'true_label': label_encoder.inverse_transform(y_test_dl),\n",
        "    'predicted_label': label_encoder.inverse_transform(y_pred_dl)\n",
        "})\n",
        "\n",
        "# Filter for misclassified comments in the LSTM model\n",
        "misclassified_df_lstm = results_df_lstm[results_df_lstm['true_label'] != results_df_lstm['predicted_label']]\n",
        "\n",
        "# Display a sample of misclassified comments from the LSTM model\n",
        "display(misclassified_df_lstm[['original_text', 'true_label', 'predicted_label']].head(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bd81709"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Assuming 'df' is your DataFrame with 'cleaned_text' and 'Label' columns\n",
        "# and you have already performed tokenization, padding, and label encoding\n",
        "# using the code from the previous LSTM model.\n",
        "\n",
        "# If you haven't run the preprocessing steps for the LSTM, please run the cell with\n",
        "# Tokenizer, pad_sequences, and LabelEncoder first.\n",
        "# For convenience, I'll include the preprocessing steps here again, assuming 'df' is available.\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000) # Adjust num_words as needed\n",
        "tokenizer.fit_on_texts(df['cleaned_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_sequence_length = 100 # Adjust based on your text length distribution\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# 1. Build the CNN model\n",
        "embedding_dim = 100 # Adjust embedding dimension as needed\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu')) # Adjust filters and kernel_size\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "model_cnn.add(Dense(64, activation='relu')) # Adjust Dense units\n",
        "model_cnn.add(Dropout(0.5)) # Adjust dropout rate\n",
        "model_cnn.add(Dense(len(label_encoder.classes_), activation='softmax')) # Output layer with number of classes\n",
        "\n",
        "# 2. Compile the model\n",
        "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history_cnn = model_cnn.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 4. Evaluate the model (optional, but recommended after training)\n",
        "loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"CNN Test Loss: {loss_cnn:.4f}\")\n",
        "print(f\"CNN Test Accuracy: {accuracy_cnn:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6054b91f"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities for the test set using the CNN model\n",
        "y_pred_prob_cnn = model_cnn.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_cnn = np.argmax(y_pred_prob_cnn, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cells\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded_cnn = label_encoder.inverse_transform(y_pred_cnn)\n",
        "\n",
        "print(\"\\nCNN Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn))\n",
        "\n",
        "print(\"\\nCNN Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "145ab419"
      },
      "source": [
        "# Task\n",
        "Explore using pre-trained word embeddings with a deep learning model on the dataset located at \"/content/Complete Data With Emoji.xlsx\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca421a29"
      },
      "source": [
        "## Load pre-trained embeddings\n",
        "\n",
        "### Subtask:\n",
        "Download and load the pre-trained GloVe word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79c6c56b"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the pre-trained GloVe embeddings and unzip the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff0d3f67"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7edf5957"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the GloVe word embeddings into a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7173560"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49945994"
      },
      "source": [
        "## Create an embedding matrix\n",
        "\n",
        "### Subtask:\n",
        "Create a matrix where each row represents a word in your vocabulary and the columns represent the embedding dimensions, using the loaded GloVe embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d5f6972"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize an embedding matrix and populate it with pre-trained GloVe embeddings based on the tokenizer's word index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b499c21a"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"Shape of embedding matrix:\", embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cb05283"
      },
      "source": [
        "## Build a deep learning model with an embedding layer\n",
        "\n",
        "### Subtask:\n",
        "Build a deep learning model (e.g., CNN or LSTM) and initialize its embedding layer with the pre-trained embedding matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4641f26e"
      },
      "source": [
        "**Reasoning**:\n",
        "Build a deep learning model (CNN) and initialize its embedding layer with the pre-trained embedding matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b357b5e4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# 1. Build the CNN model with pre-trained embeddings\n",
        "embedding_dim = 100 # This should match the dimension of your pre-trained embeddings\n",
        "vocab_size = len(tokenizer.word_index) + 1 # This should match the size of your tokenizer's vocabulary\n",
        "max_sequence_length = 100 # This should match the padding length used previously\n",
        "\n",
        "model_cnn_pretrained = Sequential()\n",
        "model_cnn_pretrained.add(Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=embedding_dim,\n",
        "                                   weights=[embedding_matrix], # Use the pre-trained embedding matrix\n",
        "                                   input_length=max_sequence_length,\n",
        "                                   trainable=False)) # Set trainable to False\n",
        "\n",
        "model_cnn_pretrained.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn_pretrained.add(GlobalMaxPooling1D())\n",
        "model_cnn_pretrained.add(Dense(64, activation='relu'))\n",
        "model_cnn_pretrained.add(Dropout(0.5))\n",
        "model_cnn_pretrained.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# 2. Compile the model\n",
        "model_cnn_pretrained.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_cnn_pretrained.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3a4539"
      },
      "source": [
        "**Reasoning**:\n",
        "The CNN model with pre-trained embeddings has been built and compiled. The next step is to train this model using the prepared data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oh8COezKV-z"
      },
      "source": [
        "# 3. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history_cnn_pretrained = model_cnn_pretrained.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941687d7"
      },
      "source": [
        "**Reasoning**:\n",
        "The CNN model with pre-trained embeddings has been trained. The next step is to evaluate its performance using appropriate metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0096703"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 4. Evaluate the model\n",
        "loss_cnn_pretrained, accuracy_cnn_pretrained = model_cnn_pretrained.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"CNN with Pre-trained Embeddings Test Loss: {loss_cnn_pretrained:.4f}\")\n",
        "print(f\"CNN with Pre-trained Embeddings Test Accuracy: {accuracy_cnn_pretrained:.4f}\")\n",
        "\n",
        "# Predict probabilities for the test set using the CNN model with pre-trained embeddings\n",
        "y_pred_prob_cnn_pretrained = model_cnn_pretrained.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_cnn_pretrained = np.argmax(y_pred_prob_cnn_pretrained, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cells\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded_cnn_pretrained = label_encoder.inverse_transform(y_pred_cnn_pretrained)\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_pretrained))\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_pretrained))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1102a65c"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the deep learning model with the pre-trained embeddings on your dataset. You can choose to keep the embedding layer weights fixed or allow them to be updated during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28a8a62"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the CNN model with pre-trained embeddings on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496e6548"
      },
      "source": [
        "# 3. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history_cnn_pretrained = model_cnn_pretrained.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6be02dc1"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained CNN model with pre-trained embeddings using appropriate metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88854a5"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 4. Evaluate the model\n",
        "loss_cnn_pretrained, accuracy_cnn_pretrained = model_cnn_pretrained.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"CNN with Pre-trained Embeddings Test Loss: {loss_cnn_pretrained:.4f}\")\n",
        "print(f\"CNN with Pre-trained Embeddings Test Accuracy: {accuracy_cnn_pretrained:.4f}\")\n",
        "\n",
        "# Predict probabilities for the test set using the CNN model with pre-trained embeddings\n",
        "y_pred_prob_cnn_pretrained = model_cnn_pretrained.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_cnn_pretrained = np.argmax(y_pred_prob_cnn_pretrained, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cells\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded_cnn_pretrained = label_encoder.inverse_transform(y_pred_cnn_pretrained)\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_pretrained))\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_pretrained))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ece69059"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained model using appropriate metrics and compare it to the models trained without pre-trained embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a08b2c1f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Pre-trained GloVe word embeddings with 100 dimensions were successfully downloaded and loaded, containing 400,000 word vectors.\n",
        "*   An embedding matrix of size (16500, 100) was created, mapping the vocabulary to the loaded GloVe embeddings.\n",
        "*   A CNN model was built with a non-trainable embedding layer initialized using the pre-trained embedding matrix.\n",
        "*   The CNN model with pre-trained embeddings was trained for 10 epochs, achieving a test accuracy of approximately 78.18%.\n",
        "*   Based on the available results, the CNN model with pre-trained embeddings (78.18% accuracy) outperformed the Logistic Regression (75.15% accuracy) and SVM (74.55% accuracy) models in terms of overall accuracy.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further hyperparameter tuning (e.g., learning rate, number of filters, kernel size) and potentially regularization techniques could be explored to improve the performance of the CNN model and mitigate potential overfitting observed during training.\n",
        "*   A detailed comparison of per-class metrics (precision, recall, F1-score) and confusion matrices for all three models (CNN with pre-trained embeddings, Logistic Regression, and SVM) is needed for a comprehensive evaluation of how pre-trained embeddings impact performance across different classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c634024"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92846956"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6722ed39"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"Shape of embedding matrix:\", embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "294c94fd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# 1. Build the CNN model with pre-trained embeddings\n",
        "embedding_dim = 100 # This should match the dimension of your pre-trained embeddings\n",
        "vocab_size = len(tokenizer.word_index) + 1 # This should match the size of your tokenizer's vocabulary\n",
        "max_sequence_length = 100 # This should match the padding length used previously\n",
        "\n",
        "model_cnn_pretrained = Sequential()\n",
        "model_cnn_pretrained.add(Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=embedding_dim,\n",
        "                                   weights=[embedding_matrix], # Use the pre-trained embedding matrix\n",
        "                                   input_length=max_sequence_length,\n",
        "                                   trainable=False)) # Set trainable to False\n",
        "\n",
        "model_cnn_pretrained.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn_pretrained.add(GlobalMaxPooling1D())\n",
        "model_cnn_pretrained.add(Dense(64, activation='relu'))\n",
        "model_cnn_pretrained.add(Dropout(0.5))\n",
        "model_cnn_pretrained.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# 2. Compile the model\n",
        "model_cnn_pretrained.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_cnn_pretrained.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74598aac"
      },
      "source": [
        "# 3. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history_cnn_pretrained = model_cnn_pretrained.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba712d95"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 4. Evaluate the model\n",
        "loss_cnn_pretrained, accuracy_cnn_pretrained = model_cnn_pretrained.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"CNN with Pre-trained Embeddings Test Loss: {loss_cnn_pretrained:.4f}\")\n",
        "print(f\"CNN with Pre-trained Embeddings Test Accuracy: {accuracy_cnn_pretrained:.4f}\")\n",
        "\n",
        "# Predict probabilities for the test set using the CNN model with pre-trained embeddings\n",
        "y_pred_prob_cnn_pretrained = model_cnn_pretrained.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_cnn_pretrained = np.argmax(y_pred_prob_cnn_pretrained, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cells\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded_cnn_pretrained = label_encoder.inverse_transform(y_pred_cnn_pretrained)\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_pretrained))\n",
        "\n",
        "print(\"\\nCNN with Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_pretrained))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39862fe8"
      },
      "source": [
        "# Task\n",
        "Explore different deep learning models and techniques, including CNN, LSTM, and pre-trained word embeddings (GloVe), to improve emoji prediction accuracy on the dataset in \"/content/Complete Data With Emoji.xlsx\". Evaluate and compare the performance of each approach using appropriate metrics and identify the best-performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcaadd5"
      },
      "source": [
        "## Experiment with trainable embeddings\n",
        "\n",
        "### Subtask:\n",
        "Modify the CNN model with pre-trained GloVe embeddings to make the embedding layer trainable and train the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9632d6a6"
      },
      "source": [
        "**Reasoning**:\n",
        "Build a new CNN model with a trainable embedding layer and train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b93ab7bb"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# 1. Build the CNN model with trainable pre-trained embeddings\n",
        "embedding_dim = 100 # This should match the dimension of your pre-trained embeddings\n",
        "vocab_size = len(tokenizer.word_index) + 1 # This should match the size of your tokenizer's vocabulary\n",
        "max_sequence_length = 100 # This should match the padding length used previously\n",
        "\n",
        "model_cnn_trainable_pretrained = Sequential()\n",
        "model_cnn_trainable_pretrained.add(Embedding(input_dim=vocab_size,\n",
        "                                             output_dim=embedding_dim,\n",
        "                                             weights=[embedding_matrix], # Use the pre-trained embedding matrix\n",
        "                                             input_length=max_sequence_length,\n",
        "                                             trainable=True)) # Set trainable to True\n",
        "\n",
        "model_cnn_trainable_pretrained.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn_trainable_pretrained.add(GlobalMaxPooling1D())\n",
        "model_cnn_trainable_pretrained.add(Dense(64, activation='relu'))\n",
        "model_cnn_trainable_pretrained.add(Dropout(0.5))\n",
        "model_cnn_trainable_pretrained.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# 2. Compile the model\n",
        "model_cnn_trainable_pretrained.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_cnn_trainable_pretrained.summary()\n",
        "\n",
        "# 3. Train the model\n",
        "# You can adjust the number of epochs and batch size\n",
        "history_cnn_trainable_pretrained = model_cnn_trainable_pretrained.fit(X_train_dl, y_train_dl, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362c686f"
      },
      "source": [
        "**Reasoning**:\n",
        "The CNN model with trainable pre-trained embeddings has been trained. The next step is to evaluate its performance using appropriate metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMpAW1c4NnHD"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 4. Evaluate the model\n",
        "loss_cnn_trainable_pretrained, accuracy_cnn_trainable_pretrained = model_cnn_trainable_pretrained.evaluate(X_test_dl, y_test_dl)\n",
        "print(f\"CNN with Trainable Pre-trained Embeddings Test Loss: {loss_cnn_trainable_pretrained:.4f}\")\n",
        "print(f\"CNN with Trainable Pre-trained Embeddings Test Accuracy: {accuracy_cnn_trainable_pretrained:.4f}\")\n",
        "\n",
        "# Predict probabilities for the test set using the CNN model with trainable pre-trained embeddings\n",
        "y_pred_prob_cnn_trainable_pretrained = model_cnn_trainable_pretrained.predict(X_test_dl)\n",
        "\n",
        "# Get the predicted labels by selecting the class with the highest probability\n",
        "y_pred_cnn_trainable_pretrained = np.argmax(y_pred_prob_cnn_trainable_pretrained, axis=1)\n",
        "\n",
        "# Decode the predicted and true labels back to their original form for better readability\n",
        "# Assuming label_encoder is still available from the previous cells\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test_dl)\n",
        "y_pred_decoded_cnn_trainable_pretrained = label_encoder.inverse_transform(y_pred_cnn_trainable_pretrained)\n",
        "\n",
        "print(\"\\nCNN with Trainable Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_trainable_pretrained))\n",
        "\n",
        "print(\"\\nCNN with Trainable Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_trainable_pretrained))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a03acd"
      },
      "source": [
        "## Evaluate cnn with trainable embeddings\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the CNN model with trainable pre-trained embeddings and compare it to previous models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c9db06"
      },
      "source": [
        "**Reasoning**:\n",
        "Compare the classification report and confusion matrix of the CNN model with trainable pre-trained embeddings to the reports from the Logistic Regression, SVM, and the CNN model with non-trainable pre-trained embeddings, focusing on all relevant metrics and then summarize the findings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7b2d158"
      },
      "source": [
        "print(\"--- Performance Comparison ---\")\n",
        "\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print(\"\\nSVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n",
        "\n",
        "# Re-evaluate the CNN model with non-trainable pre-trained embeddings for comparison\n",
        "loss_cnn_pretrained_eval, accuracy_cnn_pretrained_eval = model_cnn_pretrained.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
        "y_pred_prob_cnn_pretrained_eval = model_cnn_pretrained.predict(X_test_dl, verbose=0)\n",
        "y_pred_cnn_pretrained_eval = np.argmax(y_pred_prob_cnn_pretrained_eval, axis=1)\n",
        "y_pred_decoded_cnn_pretrained_eval = label_encoder.inverse_transform(y_pred_cnn_pretrained_eval)\n",
        "\n",
        "print(\"\\nCNN with Non-trainable Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_pretrained_eval))\n",
        "print(\"\\nCNN with Non-trainable Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_pretrained_eval))\n",
        "\n",
        "# Re-evaluate the CNN model with trainable pre-trained embeddings for comparison\n",
        "loss_cnn_trainable_pretrained_eval, accuracy_cnn_trainable_pretrained_eval = model_cnn_trainable_pretrained.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
        "y_pred_prob_cnn_trainable_pretrained_eval = model_cnn_trainable_pretrained.predict(X_test_dl, verbose=0)\n",
        "y_pred_cnn_trainable_pretrained_eval = np.argmax(y_pred_prob_cnn_trainable_pretrained_eval, axis=1)\n",
        "y_pred_decoded_cnn_trainable_pretrained_eval = label_encoder.inverse_transform(y_pred_cnn_trainable_pretrained_eval)\n",
        "\n",
        "\n",
        "print(\"\\nCNN with Trainable Pre-trained Embeddings Classification Report:\")\n",
        "print(classification_report(y_test_decoded, y_pred_decoded_cnn_trainable_pretrained_eval))\n",
        "print(\"\\nCNN with Trainable Pre-trained Embeddings Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_decoded, y_pred_decoded_cnn_trainable_pretrained_eval))\n",
        "\n",
        "# Summarize findings\n",
        "print(\"\\n--- Summary of Model Performance ---\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"CNN with Non-trainable Pre-trained Embeddings Accuracy: {accuracy_cnn_pretrained_eval:.4f}\")\n",
        "print(f\"CNN with Trainable Pre-trained Embeddings Accuracy: {accuracy_cnn_trainable_pretrained_eval:.4f}\")\n",
        "\n",
        "print(\"\\nAnalysis of Impact of Trainable Embeddings:\")\n",
        "print(\"Making the pre-trained embedding layer trainable significantly improved the performance of the CNN model.\")\n",
        "print(f\"Accuracy increased from {accuracy_cnn_pretrained_eval:.4f} (non-trainable) to {accuracy_cnn_trainable_pretrained_eval:.4f} (trainable).\")\n",
        "print(\"This suggests that fine-tuning the pre-trained embeddings on the specific dataset helps the model capture more relevant features for the emoji prediction task.\")\n",
        "\n",
        "print(\"\\nBest Performing Model:\")\n",
        "best_model = max([\n",
        "    (accuracy, \"Logistic Regression\"),\n",
        "    (accuracy_svm, \"SVM\"),\n",
        "    (accuracy_cnn_pretrained_eval, \"CNN with Non-trainable Pre-trained Embeddings\"),\n",
        "    (accuracy_cnn_trainable_pretrained_eval, \"CNN with Trainable Pre-trained Embeddings\")\n",
        "])\n",
        "print(f\"Based on overall accuracy, the best performing model is: {best_model[1]} with an accuracy of {best_model[0]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ace6c34"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Tune the hyperparameters of the best-performing model to further optimize its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "847b7494"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous evaluations, the Logistic Regression model achieved the highest overall accuracy. I will use GridSearchCV to tune its hyperparameters ('C' and 'penalty') on the resampled training data to potentially improve its performance further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9f098ff"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=1000), param_grid_lr, scoring='f1_weighted', cv=5)\n",
        "\n",
        "# Fit GridSearchCV to the resampled training data\n",
        "grid_search_lr.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best hyperparameters and the best cross-validation score\n",
        "print(\"Best hyperparameters for Logistic Regression:\", grid_search_lr.best_params_)\n",
        "print(\"Best cross-validation F1-weighted score for Logistic Regression:\", grid_search_lr.best_score_)\n",
        "\n",
        "# Train the Logistic Regression model with the best hyperparameters on the entire resampled training data\n",
        "best_lr_model = LogisticRegression(C=grid_search_lr.best_params_['C'],\n",
        "                                   penalty=grid_search_lr.best_params_['penalty'],\n",
        "                                   solver='liblinear',\n",
        "                                   max_iter=1000)\n",
        "\n",
        "best_lr_model.fit(X_train_resampled, y_train_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae19efb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The Logistic Regression model has been tuned and trained with the best hyperparameters on the resampled training data. The next step is to evaluate the performance of this fine-tuned model on the original test set using appropriate metrics and compare it to the previous results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_yYtH8zOGUe"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Evaluate the tuned Logistic Regression model on the original test set\n",
        "y_pred_tuned_lr = best_lr_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"\\nTuned Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_lr))\n",
        "\n",
        "print(\"\\nTuned Logistic Regression Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_lr))\n",
        "\n",
        "# Compare with the initial Logistic Regression model performance\n",
        "print(\"\\n--- Comparison with Initial Logistic Regression Model ---\")\n",
        "print(\"Initial Logistic Regression Accuracy:\", accuracy)\n",
        "print(\"Tuned Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_tuned_lr))\n",
        "\n",
        "print(\"\\nInitial Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nTuned Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned_lr))\n",
        "\n",
        "print(\"\\nInitial Logistic Regression Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nTuned Logistic Regression Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb6dcb5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The CNN model with trainable pre-trained GloVe embeddings achieved a test accuracy of 96.81%, significantly outperforming the CNN with non-trainable embeddings (78.08% accuracy).\n",
        "*   The Logistic Regression model initially achieved the highest overall accuracy at 97.00%.\n",
        "*   Hyperparameter tuning improved the Logistic Regression model's accuracy from 97.00% to 98.40%.\n",
        "*   The tuned Logistic Regression model showed improved precision, recall, and f1-scores, especially for minority classes.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Fine-tuning pre-trained embeddings is crucial for achieving high performance with deep learning models on this specific dataset.\n",
        "*   Hyperparameter tuning and training on resampled data effectively improved the performance of the Logistic Regression model, particularly for minority classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76d89951"
      },
      "source": [
        "# Define a mapping from labels to emojis\n",
        "emoji_map = {\n",
        "    0: 'ðŸ˜Š', # Smiling Face with Smiling Eyes\n",
        "    1: 'ðŸ˜ ', # Angry Face\n",
        "    2: 'ðŸ˜‚', # Face with Tears of Joy\n",
        "    3: 'ðŸ˜¢', # Crying Face\n",
        "    4: 'ðŸ˜¡'  # Pouting Face\n",
        "}\n",
        "\n",
        "# Function to replace '??' with the corresponding emoji based on the label\n",
        "def replace_placeholder_with_emoji(row):\n",
        "    label = row['Label']\n",
        "    text = row['Tweet_Text_With_Emoji']\n",
        "    emoji_to_add = emoji_map.get(label, '') # Get the emoji for the label, default to empty string if label not in map\n",
        "    # Replace '??' with the emoji. This assumes '??' is the placeholder.\n",
        "    # If there are other placeholders, the regex might need adjustment.\n",
        "    return text.replace('??', emoji_to_add)\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df['Tweet_Text_With_Emoji_Replaced'] = df.apply(replace_placeholder_with_emoji, axis=1)\n",
        "\n",
        "# Display the updated DataFrame with the new column\n",
        "display(df[['Tweet_Text_With_Emoji', 'Tweet_Text_With_Emoji_Replaced', 'Label']].head(100000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5eb797"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Define the input Excel file path and the output CSV file path\n",
        "excel_file_path = '/content/Complete Data With Emoji.xlsx'\n",
        "csv_file_path = '/content/Complete Data With Emoji.csv'\n",
        "\n",
        "# Read the Excel file into a pandas DataFrame\n",
        "# Make sure the file exists at this path\n",
        "try:\n",
        "    df_excel = pd.read_excel(excel_file_path)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    # Using utf-8-sig encoding is good practice for CSVs with special characters like emojis\n",
        "    df_excel.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"Successfully converted '{excel_file_path}' to '{csv_file_path}'\")\n",
        "\n",
        "    # Download the generated CSV file\n",
        "    files.download(csv_file_path)\n",
        "    print(f\"Downloading '{csv_file_path}'...\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{excel_file_path}' was not found. Please make sure it's uploaded to the /content/ directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}